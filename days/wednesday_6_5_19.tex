Today we'll have talks focusing

\subsection{Fiona McEvoy on AI-Driven Behavior Change}

{\bf Goal:} recommendations for future research on {\it decision guidance} algorithms (not decision-making algorithms). \\

$\ra$ Example: google results based on clicks/habits, recommendations systems. \\

``Nudge": any aspect of a system/intervention that alters people's behavior in a predictable way without forbidding any options or significantly changing economic incentives. Think: bench with bars on it vs. a sign. \\

{\bf Two Decision-Making Systems:} System 1 (automatic/intuition), and System 2 (rational thinking). ``Nudge" is making use of System 1. \\

\ddef{Hypernudging}{A feedback loop online that contributes heavily to how individuals make decision.}

$\ra$ Data collection about decisions and actions. \\

{\bf Point:} Hypernudging has been accused of manipulation. \\

Q: Which features keep nudges consistent with autonomous human decision-making, and which force them to be manipulative? \\

Hypothesis: the more imersive an environment is, the more they can manipulate individuals. \\

\ddef{Autonomy}{Two notions: 1) the {\it opportunity} for autonomy; Mill's freedom from manipulation, and 2) capacity for autonomy; I should be able to pursue my goals.}

One group: The environment is full of non-rational influences. So, even our most rational decisions fall out of non-rational effects. Therefore, it must be permissible to shape the choice environment. \\

$\ra$ Distinction: shaping environment vs distorting! ``Shape" is \dnote{I missed it, something like ``A intends to Y, based on Z"}, vs. distort: ``A intends to do X". \\


Features of ethically-permissible nudges:
\begin{itemize}
    \item A range of options from which to choose
    \item Can make a decision that differs from the preferences of the individual.
    \item Environment is non-deceptive/distorting/subliminal.
\end{itemize}

Three case studies:
\begin{enumerate}
    \item Online (based on Karen Yeung's work on hypernudges~\cite{yeung2017hypernudge}).
    
    $\ra$ Yeung says most online systems are manipulative in the sense that people are coerced into making non-rational decisions.
    
    \item Voice Controlled IoT: Whoever controls these has immense power: company that makes these software/hardware systems gets to choose a lot of things for you (which music service you use, shopping platform, and so on).
    
    $\ra$ Relationship between IoT/voice systems and people is becoming richer and more complex. Relationships/sensitivity develops, which masks the fact that these items are commercial entities.
    
    \item Virtual/Augmented Reality: New frontier -- nudges/distortion possible by visual/3d construction of avatars, lots of opportunity to manipulate users through the environment. \\
    
    $\ra$ Example: put elementary school kids into a VR environment, and they {\it remembered} the experience (1-2 weeks later) as a real experience (not as VR?).
    
\end{enumerate}

By the nature of all three environments, there is heavy psychological potency to manipulate users. \\

Wired: ``AR will spark the next big tech platform." $\ra$ Claim: we will use AR as the next frontier.

\spacerule

\subsection{Karen Gonzalez-Fernandez on Logic
and Heuristics in Cognitive Psychology}

Logic and heuristics have a long history: heuristics tends to be linked to the process of discovery, particularly under uncertainty and economical considerations. Conversely, logic is about deduction and certainty. \\

Q: Should logic and heuristics both be considered as part of the norms of reasoning in people? \\

{\bf Goal:} Show how logic and heuristics are understood as criteria of rationality. \\

Q: What is the role of logic/heuristics in rationality in cognitive psychology?\\

Proposal 1: ``To be rational is to reason in accordance with principles of reasoning that are based on rules of logic, probability and so forth....such rules are normative principles of reasoning"~\cite{stein1996without}. \\

Proposal 2: heuristics are ``strategies that allos us to make plausible inferences econmizing cognitive resources". \\

$\ra$ Kahneman and Tversky~\cite{kahneman2013prospect}: two systems of reasoning.
\begin{enumerate}
    \item System 1: intuitive system
    \item System 2: rational system.
\end{enumerate}

$\ra$ Heuristics appear in judgments of system 1 because they are more accessible. \\

$\ra$ Heuristics produce {\bf bias} and must be studied mainly to overcome/strategize w.r.t. systematic errors we make. \\

{\bf Gigerenzer:} proposal for ecological rationality~\cite{todd2012ecological}. In ``small worlds" only classical logics an be applied. In ``large worlds", logic can't be applied, so we need heuristics.

$\ra$ Can develop either descriptive models or normative models of heuristics. \\

{\bf Proposal:} Logic vs. heuristics as a criterion of rationality. Consider the following properties:
\begin{dtable}{ll}
{\bf Logic}&{\bf Heuristics} \\
\midrule
Logical Omniscience& Lose valid inferences \\
Infallibility& Fallibility \\
Consistency& Inconsistency \\
Context-free rules& Rules influenced by context\\
No constraints& Limitations of time/memory \\
\end{dtable}

$\ra$ Takeaway: logic and heuristics are in conflict with one another. \\

$\ra$ But, from recent work in AI, perhaps we can find a more cooperative model that incorporates both strategies. Some proposals define an algorithm as logic w/ control, that unifies aspects of both approaches. \\

{\bf Conclusion:} Logic and heuristics can cooperate. Take Gillies proposal that Inference = Logic + control, we can:
\begin{enumerate}
    \item Introduce non-classical logics
    \item Find a better understanding of processes we make when we reason in ordinary life and inside scientific methodology.
    \item Open Problem (for psychologists): Why should we maintain classical logics as a paradigm for rationality? Should we include non-classical logics in our theories of rationality?
    \item Open Problem (for philosophers): What are we doing when we do logic? How should we understand the relationship between logic and reasoning?
\end{enumerate}

\spacerule



\subsection{Keynote: Alexandru Baltag on The Topology of Surprise}

Q: Knowing the world: Can I ever know the state of the world? \\

A: Well, it depends! On three things:
\begin{enumerate}
    \item The actual world: some worlds are knowable, some not.
    \item What can I observe? Topology of observable evidence.
    \item Background information: are we given any prior knowledge?
\end{enumerate}

$\ra$ Knowability of the world sounds metaphysical and unapplied. But, in a state-space formalism, the state of the world is an abstraction: the most refined description of the world that is relevant for the given purposes. \\

$\ra$ This description {\it consists of the answer to relevant questions}. \\

\dbox{Central Question: is it possible to learn the answer to some (relevant) question, given enough observable evidence.}

Consider the following paradox:

\ddef{Surprise Exam Paradox}{A student know the date of an exam is next week. Doesn't know which day. \\

Teacher announces that the exam's date will be a surprise: even in the evening before the exam, the student will not be sure that the exam is tomorrow. (also all participants can't lie).}

But, applying backwards induction: if the announcement is true, then the test cannot take place any day of the week! So, the test cannot be a surprise and the student dismisses the announcement. But then, the test will be a surprise! Paradox. \\

Note that there are multiple interpretations here: the student might not be able to eliminate Thursday (and the rest of the days, only Friday). That is:
\begin{enumerate}
    \item Non-self-referential interpretation: teacher meant ``you wouldn't know in advance the day of the exam, without any help from me (if you are not using even the information that I am announcing now).
    
    $\ra$ Thus, the elimination process stops here.
    
    \item Self-referential interpretation (most common): teacher meant ``you will not know in advance the exam day, period, even after hearing this announcement.
    
    $\ra$ But, this version leads to a paradox (a non-lying teacher can't answer the question ``after hearing this announcement, will the exam still be a surprise?" five times (``no", specifically), and avoid contradiction.
\end{enumerate}


\ddef{Topology}{A topology on a set $\mc{X}$ is given by a family $\mathbb{T} \subseteq P(\mc{X})$, or ``states": possible descriptions of the actual world.}

So: the set of possible worlds is defined by the space $\mc{X}$. \\

$\ra$ Open sets represent the agent's evidence about the world: at world $x \in \mc{X}$, every open set $\mc{U} \in \mathbb{T}$ with $x \in \mc{U}$ is a piece of evidence. \\

Two interpretations: 1) evidence in hand (topology of actual evidence), and 2) evidence out there (topology of potential evidence). \\

Q: Why a topology? \\

A: Start with properties of the world that are directly observable, they form a topological basis. \\

Typical assumptions:
\begin{itemize}
    \item CS/Econominists tend to assume that the topology given is a {\it partition} of the state space $\mc{X}$.
    
    $\ra$ Assumes absolute certainty.
    
    \item Others tend to latch onto {\it potential} evidence, which leads to making no restrictions on the topologies.
\end{itemize}

Setting: multiple agents, multiple perspectives. This requires multiple topologies. \\

Some definitions:
\begin{enumerate}
    \item Neighborhood of a point $x \in \mc{X}$ is  any open set $\mc{U}$ where $x \in \mc{U}$.
    \item Interior point of a set $\mc{A} \subset \mc{X}$ is a point $x$ such that there is a neighborhood of $x$ where $\mc{U} \subseteq \mc{A}$.
    \item A limit point of a set A is a point $x \in \mc{X}$ s.t. every neighborhood of $x$ contains a point $y \in \mc{A}$ with $y \neq x$.
    \item Interior of A is the set of all its interior points:
    \[
    Int(A) = \{x \in \mc{X} \mid \exists U \in \mathbb{T}(x \in \mc{U} \subseteq \mc{A}\}
    \]
    \item Closure of $A$ is:
    \[
    Cl(\mc{A}) = \{x \in \mc{X} \mid \forall U \in \mathbb{T}(x \in \mc{U} \ra \mc{U} \cap \mc{A} \neq \emptyset\}
    \]
    \item The (Cantor) derivative of A is the set of its limits points:
    \[
    d(\mc{A}) := \{x \in \mc{X} \mid \forall \mc{U} \in \mathbb{T}(x \in \mc{U} \ra (\mc{U} - \{x\})\cap A \neq \emptyset\}
    \]
\end{enumerate}

Note: the interior satisfies the dual axioms, corresponding to a modal system (S4):
\begin{align}
    &Int(\mc{X}) = \mc{X}, &Int(\mc{A}) \subseteq \mc{A} \\
    &Int(\mc{A} \cap \mc{B}) = Int(\mc{A}) \cap Int(\mc{B}), &Int(Int(\mc{A})) = Int(\mc{A}).
\end{align}

Epistemic interpretation: captures main properties of a natural concept of knowledge or knowability (based roughly on the modal system S4). \\


Important Distinction: 1) you know/do not know, vs. 2) you can know/cannot know. \\

Example topologies:
\begin{enumerate}
    \item Complete ignorance: the trivial topology $\mathbb{T} = \{\emptyset, \mc{X}\}$.
    \item Omniscience (god): $\mathbb{T} = P(\mc{X}) = \{\mc{Y} \mid \mc{Y} \subseteq \mc{X}\}$
    \item Knowledge based on measurements of points on a line.
\end{enumerate}

So far: we have a proper definition of knowledge and knowability. Let us return to the question: \\

Q: Is the actual state of the world known? \\

A: Depends on evidence. Suppose background information is given by a subset $\mc{A} \subseteq \mc{X}$ of a topological space. \\

$\ra$ Suppose: actual world $x$ belongs to $\mc{A}$. Then, I can know actual world $x$ iff $x$ is isolated in $A$, that is, iff $\{x\} = \mc{U} \cap \mc{A}$ for some open set $\mc{U} \in \tau_\mc{X}$.\\

{\bf Example:} Policeman and the Speeding Car.
\begin{itemize}
    \item Policeman uses a radar with accuracy $\pm$ 2 mph to determine whether the car is speeding in a 50mph zone.
    \item Radar shows 51 mph.
    \item $\mc{X} = (0, \infty)$ is the set of possible worlds where we assume the car is known to be moving.
    \item The property ``speeding" is knowable, but it is now known in this context.
\end{itemize}

{\bf Example:} Teacker marks a point $x$ on a real line $R$. Announces that the point is in the set:
\begin{align}
A = &\{0\}\ \cup OR\\
&\{\frac{1}{n} : n \in \mathbb{N}, n \geq 2\}\ OR \\ &\{\frac{1}{n} + \frac{1}{n^m} : n,m \in \mathbb{N}_{\geq 2}\}\ OR\\
&[1,2].
\end{align}

Q: Can the student know the position? Depends on further measurements, their accuracy, and so on. \\

$\ra$ We can bury a surprise exam paradox in here, too. \\

Let's revisit a topological epistemic analysis of the surprise exam paradox: what is the potential evidence in this context? \\

Under the self-referential view, we can recreate the paradox. The topological analysis gave us some justification as to {\it why} this is a paradox: the only perfect subset of $\mc{A}$ is the empty set! \\

Multi-agent example: Two numbers, one on each of two agents' foreheads. The numbers are off by one. \\

$\ra$ Ask each agent: do you know the number on your head? First time: no! Ask again: no! ask again: yes! Can use Cantor's derivative method to repeatedly eliminate possible worlds until $x$ is known (the true state of the world).
\spacerule





\subsection{David Fernandez-Duque on Stratified Evidence Logic}

Example: Alice wants to know if the movie ``Roma" by Alfonso Cuaron is a good movie. She asks her chatbot, Bot. Bot decides that wikipedia and rotten tomatoes are not sufficient for answering Alice's question. \\

Bot does conclude that an oscar nomination and a rotten tomatoes score of 90\% or higher are sufficient to answer Alice's question. \\

$\ra$ Both can only answer her question with $N = 2$ pieces of evidence. But what if $N$ is really large? $N > 10,000$? \\

{\bf Goal:} Present a logical framework in which the amount of evidence needed to conclude a new belief can be explicitly counted in order to model {\it bounded rationality}. \\

\ddef{Evidence Models}{An evidence model consists of:
\begin{enumerate}
    \item Possible worlds where atomic facts may attain different truth values.
    \item Evidence sets: sets of worlds that remain possible after making an individual observation.
\end{enumerate}}

Some evidence-based situations:
\begin{itemize}
    \item $Ap$: $p$ is true everywhere
    \item $Ep$: Some piece of evidence supports that $p$.
    \item $\square_0 p$: Some factual piece of evidence supports that $p$.
    \item $\square p$: There is some finite collection of factual evidence supporting $p$.
    \item $[\alpha]p$: there is a collection of size less than $\alpha + 1$ that supports $p$.
    
    $\ra$ This is the main contribution of the work: a new theory for clarifying the size of evidence and its role.
\end{itemize}

\newpage
\ddef{Stratified Evidence Model}{A stratified evidence model is a triple, $M = (W, (E_\alpha)_{\alpha \in \mathbb{N}}, V)$, where the evidence all has to be arranged as less than a particular size/effort (as denoted by $\alpha$).}

Stratified Evidence logic:
\begin{enumerate}
    \item Syntax: $\perp, p, \phi \ra \psi, [\alpha]_\phi$, where $\alpha$ is a cardinal/natural number, $p$ is an atomic proposition.
    \item New piece: what happens to stratified evidence? $(M, w) \implies [\alpha]_\phi$ if there is $X$ that support $\phi$ subject to the $\alpha$ constraint.
\end{enumerate}

Back to the Roma Example: Bot needed to check Rotten Tomatoes and Wikipedia to verify that Roma is a great movie. This can be represented in a strict stratified evidence model. \\

Example 2: Let's consider graph problems, like determining whether a graph contains a hamiltonian cycle. \\

Q: How might we express this problem in stratified epistemic logic? \\

A: Well, we pose statements like $M \implies h \ra [1]h$, if one piece of evidence can let us answer the decision problem. More generally, we'd see $M \implies \neg h \ra [\omega]\neg h$, where $\omega$ is the first infinite cardinal. \\

Example 3: The halting problem. $H_n$ is the set of machines that halt in at most $n$ steps, $W$ is the set of all turing machines. Then we can express this: $M \implies h \ra [\omega]h$. To determine if they don't halt, though, we of course need to write $M \implies \neg p \ra [\omega_1]\neg p$, where $\omega_1$ is the first uncountable ordinal (since we would have to run the machine forever).\\

Main result:
\begin{theorem}
The following are equivalent:
\begin{enumerate}
    \item $\phi$ is satisfiable over a stratified evidence model
    \item $\phi$ is satisfiable over a strict stratified evidence model
    \item $\phi$ is satisfiable over a finite stratified evidence model.
\end{enumerate}
\end{theorem}

Expressing standard evidence notions:
\begin{enumerate}
    \item $A\phi \equiv [0]_\phi$: $\phi$ is true everywhere
    \item $E \phi \equiv \langle 0\rangle [1]_\phi$: There is evidence supporting $\phi$.
    \item $\square_0 \phi \equiv [1]_\phi$: There is factual evidence supporting $\phi$.
\end{enumerate}

Concluding Remarks:
\begin{itemize}
    \item Stratified evidence logics allow us to explicitly quantify the amount/cost of evidence required to justify a given fact.
    \item This logic retains the nice computational behavior of standard evidence logics.
    
    Q: What is precise computational complexity of stratified evidence logic and its fragments?
    
    \item Many familiar evidence-based notions naturally fall into our framework.
    
    \item Evidence may also be stratified by trust rather than effort, leading to very different logics.
\end{itemize}

\spacerule

\subsection{Armando Castaneda on Distributed
Computing with Bounded Rational Agents}

{\bf Goal:} Think about distributed systems from an agent-centric view. \\

Two components of a distributed system:
\begin{enumerate}
    \item Agents: computers, robots, people, ants.
    \item Communication medium: messages, signals, public/private announcements.
\end{enumerate}

$\ra$ We have loads of models, all of which make different assumptions; timing assumptions (synchronous/asynchronous), local failure assumptions, static/dynamic, and so on. \\

Q: What about {\it local} computations?\\

A: Two views: 
\begin{enumerate}
    \item Computability approach: each agent is ``more" than a Turing machine
    
    $\ra$ can be useful to clarify impossibility results. But, suffers because some solvable tasks are not implementable in reality, period.
    
    
    \item Algorithmic approach: TM with unbounded time/space, often a loose definition.
    
    $\ra$ People do not like locally expensive solutions (usually assume agents can solve NP-Complete problems locally).
\end{enumerate}

\dbox{{\bf New Proposal:} Incorporate realistic models of local computations in the study of distributed systems.}

$\ra$ Bound resources like time, space, and bandwidth. \\

Compact Local Streaming (CLS) Model: Agent is a turing machine with {\it compact space}, where space $\approx \log k$, where $k$ is the number of agents in the system. \\

Communication: synchronous failure free (SFF):
\begin{enumerate}
    \item Communication graph with arbitrary topology.
    \item Message-passing to neighbors
    \item Synchronous communication
    \item Nobody knows the whole graph
    \item Failure free
    \item Execution/communication proceeds in discrete rounds.
\end{enumerate}

Target: simple and general framework for studying these notions. \\



Local Streaming Algorithms: each round, agents 1) send message to neighbors, 2) receive messages from neighbors, 3) perform local computations. \\

$\ra$ Because of the space limitation imposed on agents, must treat step (2) like a streaming algorithm. \\

Two parts to each algorithm: 1) local streaming algorithm for reacting to messages, and 2) compact communication protocol. \\

{\bf Positive Results:} Lots of possible results on solving various problems using this model: DFS, BFS, leader election, $\Delta+1$ graph coloring, $k$-th largest element, compact routing, and so on. \\

{\bf Negative Results:} Recall that the size of local state is bounded, size of message is bounded, and the memory of the whole system is bounded. So, there are loads of problems that can't be solved. CLS has essentially the power of a TM with poly space. So, problems that require exponential space cannot be solved in CLS. \\

Future Directions:
\begin{enumerate}
    \item Continued analysis of CLS, complexity separations, epistemic logics, and so on.
    \item Resource-bounded agents in other classic models.
    \item Impact of CLS to classical results.
\end{enumerate}

\spacerule
\subsection{Fernando Velazquez-Quesada on Reliability-Based Preference Dynamics}

{\bf Main Question:} How do other people influence us? More precisely, how do the preferences and beliefs of other people influence our individual preferences and beliefs? \\

Proposals:
\begin{itemize}
    \item Reaching a consensus from~\citet{degroot1974reaching}: agents have some probability distribution and a weight for other agents.
    
    $\ra$ My revised distribution is some linear combination of everybody's distributions.
    
    \item Dynamics of Peer Pressure
    
    \item Threshold Models
\end{itemize}

$\ra$ Lots of perspectives on this phenomena. Generally, we want to understand this process. \\

This proposal: {\it reliability-based} preference change. \\

Q: How do other people's preferences affect ours when you have a priority ordering among them? \\

Setting: Finite set of agents, each with a preference relation over situations and a reliability relation over agents. \\

Goal: provide a function from everybody's preferences and a priority among them to a single preference relation using a {\it lexicographic rule}. \\


Approach:
\begin{enumerate}
    \item Lexicographic rule
    \item Model operation for simultaneous preference update
    \item Modality describing the effect (recursion-axioms based axiom system)
    \item Some variations of the operation
    \item Identifying situations where the iteration leads to unanimity/stability.
\end{enumerate}


\ddef{PR Frame}{The $i$-th agent will be associated with a tuple $(W,f_i,r_i)$ where:
\begin{enumerate}
    \item $W$ is a finite set of worlds (that agents will have preferences over).
    \item $f_i$: a preference relation (an ordering on worlds
    \item $r_i$: A reliability relation (ordering over other agents.
\end{enumerate}}

Also need a language, semantic interpretation, and axioms for describing the worlds/models (it's roughly first order logic, with the added machinery to express preference/reliability relations). \\


Example 1 (Total order case): Give precedence to the most important ordering/agent. Can generate new preferences. \\

Example 2 (partial order case): Solutions already in literature, this formalism can roughly capture these formalisms. \\

$\ra$ Can also handle the preorder case.

\dnote{Had to run for the day!}